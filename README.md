# Neural Networks From Scratch — End‑to‑End Mini Project

- A single neuron and a linear perceptron (AND, XOR)
- A feed‑forward neural network (2‑layer MLP) with backpropagation
- Activation functions: sigmoid, tanh, ReLU and softmax (for multi‑class)
- Why linear neurons are limited (XOR)
- Information theory: entropy, cross‑entropy, and KL divergence, and how they relate
- End‑to‑end training on toy datasets with visualizations
- verything is implemented from scratch using only NumPy + matplotlib.
